apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: collect-qc-data-templates
spec:
  templates:
    - name: collect-qc-data-bam
      inputs:
        parameters:
          - name: sample-id # not Uuid but user defined String value.
          - name: run-id
          - name: metrics-dir
          - name: data-source-type
            default: "dna"
      volumes:
        - name: metrics-dir
          hostPath:
            path: "{{inputs.parameters.metrics-dir}}"
            type: Directory
      script:
        name: "collect-qc-data-bam"
        image: demisto/pandas:1.0.0.80782
        # includes 'requests' package
        workingDir: /mnt/metrics/
        volumeMounts:
          - name: metrics-dir
            mountPath: /mnt/metrics/
        command: [python]
        source: |
          import os
          import pandas as pd
          import json
          import requests

          # cluster info
          token = os.environ["GENNEXT_CLUSTER_TOKEN"]
          addr = "gennext-backend.gennext-dev.svc.cluster.local:3030"
          url = f"http://{addr}/api/v0/runs/{{inputs.parameters.run-id}}/quality"

          # collect input data
          sample_id = "{{inputs.parameters.sample-id}}"
          multiqc_data = pd.read_csv("multiqc_data/multiqc_general_stats.txt", sep="\t")

          qc_data = {
              "1x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-1_x_pc'].values[0],
              "5x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-5_x_pc'].values[0],
              "10x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-10_x_pc'].values[0],
              "30x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-30_x_pc'].values[0],
              "50x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-50_x_pc'].values[0],
              "mean_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-mean_coverage'].values[0],
              "median_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-median_coverage'].values[0],
              "contamination": multiqc_data[multiqc_data['Sample'] == sample_id]['verifybamid-FREEMIX'].values[0],
          }

          qc_data_dict = {
              "dataSourceType": "{{inputs.parameters.data-source-type}}",
              "1X": str(f'{qc_data["1x_coverage"]:.2f}'),
              "5X": str(f'{qc_data["5x_coverage"]:.2f}'),
              "10X": str(f'{qc_data["10x_coverage"]:.2f}'),
              "30X": str(f'{qc_data["30x_coverage"]:.2f}'),
              "50X": str(f'{qc_data["50x_coverage"]:.2f}'),
              "meanCoverage": str(f'{qc_data["mean_coverage"]:.2f}'),
              "medianCoverage": str(f'{qc_data["median_coverage"]:.2f}'),
              "contamination": str(f'{qc_data["contamination"]:.5f}'),
              "totalReads": None,
              "avgReadLength": None,
              "gender": None, #TODO
              "roh": None # TODO
          }

          # if fqs_fnames_str != "":
          #     fqs_fnames = json.loads(fqs_fnames_str)
          #     fqs_bnames  = list(map(lambda fq: ".".join(fq.split(".")[:-2]) if fq.endswith(".gz") else ".".join(fq.split(".")[:-1]), fqs_fnames))
          #     qc_data_fqs = {
          #         "total_sequences": int(sum(multiqc_data[multiqc_data['Sample'].isin(fqs_bnames)]['fastqc-total_sequences'])/len(fqs_bnames)),
          #         "avg_sequence_length": sum(multiqc_data[multiqc_data['Sample'].isin(fqs_bnames)]['fastqc-avg_sequence_length'])/len(fqs_bnames)
          #     }
          #     qc_data_dict['totalReads'] = int(qc_data_fqs["total_sequences"])
          #     qc_data_dict['avgReadLength'] = str(f'{qc_data_fqs["avg_sequence_length"]:.2f}')

          # qc_data_json = json.dumps(qc_data_dict)

          print(json.dumps({"runId": "{{inputs.parameters.run-id}}", "sampleId": "{{inputs.parameters.sample-id}}", "qcData": qc_data_dict, "token": token }))

          res = requests.post(url, json={"runId": "{{inputs.parameters.run-id}}", "sampleId": "{{inputs.parameters.sample-id}}", "qcData": qc_data_dict, "token": token })
          if res.status_code != 200:
              raise Exception(res.text)
        env:
          - name: GENNEXT_CLUSTER_TOKEN
            valueFrom:
              secretKeyRef:
                name: gennext-cluster-secrets
                key: gennext_cluster_token
    - name: collect-qc-data
      inputs:
        parameters:
          - name: sample-id # not Uuid but user defined String value.
          - name: run-id
          - name: sample-fqs-fnames
          - name: metrics-dir
          - name: data-source-type
            default: "dna"
      volumes:
        - name: metrics-dir
          hostPath:
            path: "{{inputs.parameters.metrics-dir}}"
            type: Directory
      script:
        name: "collect-qc-data"
        image: demisto/pandas:1.0.0.80782
        # includes 'requests' package
        workingDir: /mnt/metrics/
        volumeMounts:
          - name: metrics-dir
            mountPath: /mnt/metrics/
        command: [python]
        source: |
          import os
          import pandas as pd
          import json
          import requests
          import zipfile
          import re  # Added missing import for regex

          # cluster info
          token = os.environ["GENNEXT_CLUSTER_TOKEN"]
          addr = "gennext-backend.gennext-dev.svc.cluster.local:3030"
          url = f"http://{addr}/api/v0/runs/{{inputs.parameters.run-id}}/quality"

          # collect input data
          sample_id = "{{inputs.parameters.sample-id}}"
          multiqc_data = pd.read_csv("multiqc_data/multiqc_general_stats.txt", sep="\t")
          fqs_fnames_str = '{{inputs.parameters.sample-fqs-fnames}}'

          def parse_fastqc_stats(fq):
              try:  # Added error handling for file access
                  with zipfile.ZipFile(f'fastqc/pre-trim/{fq}_fastqc.zip', 'r') as zip_ref:
                      with zip_ref.open(f'{fq}_fastqc/fastqc_data.txt') as file:
                          content = file.read().decode('utf-8')

                  # Find the Basic Statistics section
                  match = re.search(r'>>Basic Statistics.*?>>END_MODULE', content, re.DOTALL)
                  if not match:
                      return {}

                  # Parse only relevant lines into a dictionary
                  result = {}
                  for line in match.group(0).split('\n'):
                      if line and not line.startswith('>>') and not line.startswith('#'):
                          parts = line.split('\t')
                          if len(parts) >= 2:
                              key = parts[0].strip()
                              if key == 'Total Sequences':
                                  result[key] = int(parts[1])
                              elif key == 'Total Bases':
                                  result[key] = float(parts[1].split()[0])
                              elif key == '%GC':
                                  result[key] = int(parts[1])
                              else:
                                  result[key] = parts[1].strip()
                  return result
              except Exception as e:
                  print(f"Error parsing FastQC stats for {fq}: {str(e)}")
                  return {}

          # Get QC data from multiqc
          try:
              qc_data = {
                  "1x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-1_x_pc'].values[0],
                  "5x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-5_x_pc'].values[0],
                  "10x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-10_x_pc'].values[0],
                  "30x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-30_x_pc'].values[0],
                  "50x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-50_x_pc'].values[0],
                  "mean_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-mean_coverage'].values[0],
                  "median_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-median_coverage'].values[0],
                  "contamination": multiqc_data[multiqc_data['Sample'] == sample_id]['verifybamid-FREEMIX'].values[0],
              }
          except IndexError:
              print(f"Error: Sample ID '{sample_id}' not found in multiqc data")
              raise Exception(f"Sample ID '{sample_id}' not found in multiqc data")

          qc_data_dict = {
              "dataSourceType": "{{inputs.parameters.data-source-type}}",
              "1X": str(f'{qc_data["1x_coverage"]:.2f}'),
              "5X": str(f'{qc_data["5x_coverage"]:.2f}'),
              "10X": str(f'{qc_data["10x_coverage"]:.2f}'),
              "30X": str(f'{qc_data["30x_coverage"]:.2f}'),
              "50X": str(f'{qc_data["50x_coverage"]:.2f}'),
              "meanCoverage": str(f'{qc_data["mean_coverage"]:.2f}'),
              "medianCoverage": str(f'{qc_data["median_coverage"]:.2f}'),
              "contamination": str(f'{qc_data["contamination"]:.5f}'),
              "totalReads": None,
              "avgReadLength": None,
              "gender": None, #TODO
              "roh": None # TODO
          }

          if fqs_fnames_str != "":
              try:
                  fqs_fnames = json.loads(fqs_fnames_str)
                  fqs_bnames = []
                  for fq in fqs_fnames:
                      if fq.endswith(".gz"):
                          # Extract basename without the last two extensions (.fastq.gz)
                          fq_basename = ".".join(fq.split(".")[:-2])
                      else:
                          # Extract basename without the last extension (.fastq)
                          fq_basename = ".".join(fq.split(".")[:-1])
                      fqs_bnames.append(os.path.basename(fq_basename))

                  all_stats = {fq: parse_fastqc_stats(fq) for fq in fqs_bnames}
                  valid_stats = {f: stats for f, stats in all_stats.items() if stats and 'Total Sequences' in stats and 'Sequence length' in stats}

                  if valid_stats:
                      qc_data_fqs = {
                          "total_sequences": int(sum([int(valid_stats[f]['Total Sequences']) for f in valid_stats])/len(valid_stats)),
                          "avg_sequence_length": int(sum([int(valid_stats[f]['Sequence length']) for f in valid_stats])/len(valid_stats)),
                          "file_names": ', '.join(sorted([valid_stats[f].get('Filename', f) for f in valid_stats]))
                      }
                      qc_data_dict['totalReads'] = qc_data_fqs["total_sequences"]
                      qc_data_dict['avgReadLength'] = str(f'{qc_data_fqs["avg_sequence_length"]:.2f}')
              except Exception as e:
                  print(f"Error processing FASTQ files: {str(e)}")
                  # Continue without FASTQ stats rather than failing completely

          # Fix URL template parameter
          url = f"http://{addr}/api/v0/runs/{{inputs.parameters.run-id}}/quality".replace("{{inputs.parameters.run-id}}", "{{inputs.parameters.run-id}}")

          # Send request with properly formatted data
          try:
              res = requests.post(
                  url,
                  json={
                      "runId": "{{inputs.parameters.run-id}}",
                      "sampleId": "{{inputs.parameters.sample-id}}",
                      "qcData": qc_data_dict,
                      "token": token
                  },
                  timeout=30  # Add timeout to prevent hanging
              )
              if res.status_code != 200:
                  raise Exception(f"API request failed: {res.status_code} - {res.text}")
              print(f"Successfully uploaded QC data for sample {sample_id}")
          except requests.exceptions.RequestException as e:
              raise Exception(f"API connection error: {str(e)}")
        env:
          - name: GENNEXT_CLUSTER_TOKEN
            valueFrom:
              secretKeyRef:
                name: gennext-cluster-secrets
                key: gennext_cluster_token
    - name: collect-qc-data-on-node
      nodeSelector:
        kubernetes.io/hostname: "{{inputs.parameters.node-name}}"
      inputs:
        parameters:
          - name: sample-id # not Uuid but user defined String value.
          - name: run-id
          - name: sample-fqs-fnames
          - name: metrics-dir
          - name: data-source-type
            default: "dna"
          - name: node-name
      volumes:
        - name: metrics-dir
          hostPath:
            path: "{{inputs.parameters.metrics-dir}}"
            type: Directory
      script:
        name: "collect-qc-data"
        image: demisto/pandas:1.0.0.80782
        # includes 'requests' package
        workingDir: /mnt/metrics/
        volumeMounts:
          - name: metrics-dir
            mountPath: /mnt/metrics/
        command: [python]
        source: |
          import os
          import pandas as pd
          import json
          import requests
          import zipfile
          import re  # Added missing import for regex

          # cluster info
          token = os.environ["GENNEXT_CLUSTER_TOKEN"]
          addr = "gennext-backend.gennext-dev.svc.cluster.local:3030"
          url = f"http://{addr}/api/v0/runs/{{inputs.parameters.run-id}}/quality"

          # collect input data
          sample_id = "{{inputs.parameters.sample-id}}"
          multiqc_data = pd.read_csv("multiqc_data/multiqc_general_stats.txt", sep="\t")
          fqs_fnames_str = '{{inputs.parameters.sample-fqs-fnames}}'

          def parse_fastqc_stats(fq):
            # check if fastqc prefix folder exists
            fatqc_prefix = "fastqc/pre-trim/"
            if not os.path.exists(fatqc_prefix):
              fatqc_prefix = "fastqc/"
            try:  # Added error handling for file access
              with zipfile.ZipFile(f'{fatqc_prefix}{fq}_fastqc.zip', 'r') as zip_ref:
                with zip_ref.open(f'{fq}_fastqc/fastqc_data.txt') as file:
                  content = file.read().decode('utf-8')

              # Find the Basic Statistics section
              match = re.search(r'>>Basic Statistics.*?>>END_MODULE', content, re.DOTALL)
              if not match:
                return {}

              # Parse only relevant lines into a dictionary
              result = {}
              for line in match.group(0).split('\n'):
                if line and not line.startswith('>>') and not line.startswith('#'):
                  parts = line.split('\t')
                  if len(parts) >= 2:
                    key = parts[0].strip()
                    if key == 'Total Sequences':
                      result[key] = int(parts[1])
                    elif key == 'Total Bases':
                      result[key] = float(parts[1].split()[0])
                    elif key == '%GC':
                      result[key] = int(parts[1])
                    else:
                      result[key] = parts[1].strip()
              return result
            except Exception as e:
              print(f"Error parsing FastQC stats for {fq}: {str(e)}")
              return {}

          # Get QC data from multiqc
          try:
            qc_data = {
              "1x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-1_x_pc'].values[0],
              "5x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-5_x_pc'].values[0],
              "10x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-10_x_pc'].values[0],
              "30x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-30_x_pc'].values[0],
              "50x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-50_x_pc'].values[0],
              "mean_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-mean_coverage'].values[0],
              "median_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-median_coverage'].values[0],
              "contamination": multiqc_data[multiqc_data['Sample'] == sample_id]['verifybamid-FREEMIX'].values[0],
            }
          except IndexError:
            print(f"Error: Sample ID '{sample_id}' not found in multiqc data")
            raise Exception(f"Sample ID '{sample_id}' not found in multiqc data")

          qc_data_dict = {
              "dataSourceType": "{{inputs.parameters.data-source-type}}",
              "1X": str(f'{qc_data["1x_coverage"]:.2f}'),
              "5X": str(f'{qc_data["5x_coverage"]:.2f}'),
              "10X": str(f'{qc_data["10x_coverage"]:.2f}'),
              "30X": str(f'{qc_data["30x_coverage"]:.2f}'),
              "50X": str(f'{qc_data["50x_coverage"]:.2f}'),
              "meanCoverage": str(f'{qc_data["mean_coverage"]:.2f}'),
              "medianCoverage": str(f'{qc_data["median_coverage"]:.2f}'),
              "contamination": str(f'{qc_data["contamination"]:.5f}'),
              "totalReads": None,
              "avgReadLength": None,
              "gender": None, #TODO
              "roh": None # TODO
          }

          if fqs_fnames_str != "":
            try:
              fqs_fnames = json.loads(fqs_fnames_str)
              fqs_bnames = []
              for fq in fqs_fnames:
                if fq.endswith(".gz"):
                  # Extract basename without the last two extensions (.fastq.gz)
                  fq_basename = ".".join(fq.split(".")[:-2])
                else:
                  # Extract basename without the last extension (.fastq)
                  fq_basename = ".".join(fq.split(".")[:-1])
                fqs_bnames.append(os.path.basename(fq_basename))

              all_stats = {fq: parse_fastqc_stats(fq) for fq in fqs_bnames}
              valid_stats = {f: stats for f, stats in all_stats.items() if stats and 'Total Sequences' in stats and 'Sequence length' in stats}

              if valid_stats:
                qc_data_fqs = {
                  "total_sequences": int(sum([int(valid_stats[f]['Total Sequences']) for f in valid_stats])/len(valid_stats)),
                  "avg_sequence_length": int(sum([int(valid_stats[f]['Sequence length'].split('-')[-1]) for f in valid_stats])/len(valid_stats)),
                  "file_names": ', '.join(sorted([valid_stats[f].get('Filename', f) for f in valid_stats]))
                }
                qc_data_dict['totalReads'] = qc_data_fqs["total_sequences"]
                qc_data_dict['avgReadLength'] = str(f'{qc_data_fqs["avg_sequence_length"]:.2f}')
            except Exception as e:
                print(f"Error processing FASTQ files: {str(e)}")
                # Continue without FASTQ stats rather than failing completely

          # Fix URL template parameter
          url = f"http://{addr}/api/v0/runs/{{inputs.parameters.run-id}}/quality".replace("{{inputs.parameters.run-id}}", "{{inputs.parameters.run-id}}")

          # Send request with properly formatted data
          try:
            res = requests.post(
              url,
              json={
                "runId": "{{inputs.parameters.run-id}}",
                "sampleId": "{{inputs.parameters.sample-id}}",
                "qcData": qc_data_dict,
                "token": token
              },
              timeout=30  # Add timeout to prevent hanging
            )
            if res.status_code != 200:
              raise Exception(f"API request failed: {res.status_code} - {res.text}")
            print(f"Successfully uploaded QC data for sample {sample_id}")
          except requests.exceptions.RequestException as e:
            raise Exception(f"API connection error: {str(e)}")
        env:
          - name: GENNEXT_CLUSTER_TOKEN
            valueFrom:
              secretKeyRef:
                name: gennext-cluster-secrets
                key: gennext_cluster_token
    # - name: collect-qc-data-on-node
    #   nodeSelector:
    #     kubernetes.io/hostname: "{{inputs.parameters.node-name}}"
    #   inputs:
    #     parameters:
    #       - name: sample-id # not Uuid but user defined String value.
    #       - name: run-id
    #       - name: sample-fqs-fnames
    #         default: ""
    #       - name: metrics-dir
    #       - name: node-name
    #   volumes:
    #     - name: metrics-dir
    #       hostPath:
    #         path: "{{inputs.parameters.metrics-dir}}"
    #         type: Directory
    #   script:
    #     name: "collect-qc-data"
    #     image: demisto/pandas:1.0.0.80782
    #     # includes 'requests' package
    #     workingDir: /mnt/metrics/
    #     volumeMounts:
    #       - name: metrics-dir
    #         mountPath: /mnt/metrics/
    #     command: [python]
    #     source: |
    #       import os
    #       import pandas as pd
    #       import json
    #       import requests

    #       # cluster info
    #       token = os.environ["GENNEXT_CLUSTER_TOKEN"]
    #       addr = "gennext-backend.gennext-dev.svc.cluster.local:3030"
    #       url = f"http://{addr}/api/v0/runs/{{inputs.parameters.run-id}}/quality"

    #       # collect input data
    #       sample_id = "{{inputs.parameters.sample-id}}"
    #       multiqc_data = pd.read_csv("multiqc_data/multiqc_general_stats.txt", sep="\t")
    #       fqs_fnames_str = '{{inputs.parameters.sample-fqs-fnames}}'

    #       qc_data = {
    #           "1x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-1_x_pc'].values[0],
    #           "5x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-5_x_pc'].values[0],
    #           "10x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-10_x_pc'].values[0],
    #           "30x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-30_x_pc'].values[0],
    #           "50x_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-50_x_pc'].values[0],
    #           "mean_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-mean_coverage'].values[0],
    #           "median_coverage": multiqc_data[multiqc_data['Sample'] == sample_id]['mosdepth-median_coverage'].values[0],
    #           "contamination": multiqc_data[multiqc_data['Sample'] == sample_id]['verifybamid-FREEMIX'].values[0],
    #       }

    #       qc_data_dict = {
    #           "1X": f'{qc_data["1x_coverage"]:.2f}',
    #           "5X": f'{qc_data["5x_coverage"]:.2f}',
    #           "10X": f'{qc_data["10x_coverage"]:.2f}',
    #           "30X": f'{qc_data["30x_coverage"]:.2f}',
    #           "50X": f'{qc_data["50x_coverage"]:.2f}',
    #           "meanCoverage": f'{qc_data["mean_coverage"]:.2f}',
    #           "medianCoverage": f'{qc_data["median_coverage"]:.2f}',
    #           "contamination": f'{qc_data["contamination"]:.5f}',
    #           "totalReads": None,
    #           "avgReadLength": None,
    #           "gender": None, #TODO
    #           "roh": None # TODO
    #       }

    #       if fqs_fnames_str != "":
    #           fqs_fnames = json.loads(fqs_fnames_str)
    #           fqs_bnames  = list(map(lambda fq: ".".join(fq.split(".")[:-2]) if fq.endswith(".gz") else ".".join(fq.split(".")[:-1]), fqs_fnames))
    #           qc_data_fqs = {
    #               "total_sequences": int(sum(multiqc_data[multiqc_data['Sample'].isin(fqs_bnames)]['fastqc-total_sequences'])/len(fqs_bnames)),
    #               "avg_sequence_length": sum(multiqc_data[multiqc_data['Sample'].isin(fqs_bnames)]['fastqc-avg_sequence_length'])/len(fqs_bnames)
    #           }
    #           qc_data_dict['totalReads'] = int(qc_data_fqs["total_sequences"])
    #           qc_data_dict['avgReadLength'] = f'{qc_data_fqs["avg_sequence_length"]:.2f}'

    #       # qc_data_json = json.dumps(qc_data_dict)

    #       print(json.dumps({"runId": "{{inputs.parameters.run-id}}", "sampleId": "{{inputs.parameters.sample-id}}", "qcData": qc_data_dict, "token": token }))

    #       res = requests.post(url, json={"runId": "{{inputs.parameters.run-id}}", "sampleId": "{{inputs.parameters.sample-id}}", "qcData": qc_data_dict, "token": token })
    #       if res.status_code != 200:
    #           raise Exception(res.text)
    #     env:
    #       - name: GENNEXT_CLUSTER_TOKEN
    #         valueFrom:
    #           secretKeyRef:
    #             name: gennext-cluster-secrets
    #             key: gennext_cluster_token
